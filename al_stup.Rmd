---
title: "stupid backoff"
author: "Yigit Ozan Berk"
date: "11/16/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(quanteda)
library(dplyr)
library(stringr)
library(wordcloud)
library(data.table)
library(readtext)
library(spacyr)
library(ggplot2)
library(sqldf)
```



# Downloading Data

The dataset can be downloaded from a link given in the course website. [https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip]  

The unzipped file contains a directory called final, then a subdirectory called en_US, which contains the texts.

There are 3 text files.
* en_US.blogs.txt - text from blog posts
* en_US.news.txt - text from news articles
* en_US.twitter.txt - tweets on Twitter

# Introduction

The goal here to display the initial data exploration, and show I am on the track to extablishing my algorithm. 

```{r}
numwords <- system("wc -w *.txt", intern=TRUE)  # intern=TRUE to return output  
numlines <- system("wc -l *.txt", intern=TRUE)
numbytes <- system("wc -c *.txt", intern=TRUE)

# number of words for each dataset
blog.numwords <- as.numeric(gsub('[^0-9]', '', numwords[1]))
news.numwords <- as.numeric(gsub('[^0-9]', '', numwords[2]))
twit.numwords <- as.numeric(gsub('[^0-9]', '', numwords[3]))

# number of lines for each dataset
blog.numlines <- as.numeric(gsub('[^0-9]', '', numlines[1]))
news.numlines <- as.numeric(gsub('[^0-9]', '', numlines[2]))
twit.numlines <- as.numeric(gsub('[^0-9]', '', numlines[3]))

# number of bytes for each dataset
blog.numbytes <- as.numeric(gsub('[^0-9]', '', numbytes[1]))
news.numbytes <- as.numeric(gsub('[^0-9]', '', numbytes[2]))
twit.numbytes <- as.numeric(gsub('[^0-9]', '', numbytes[3]))

words = rbind(blog.numwords, news.numwords, twit.numwords)
lines = rbind(blog.numlines, news.numlines, twit.numlines)
bytes = rbind(blog.numbytes, news.numbytes, twit.numbytes)

data.frame(words = words, lines= lines, Mb = bytes/1000000, 
           row.names = c("blog", "news", "twit"))
```

For memory storage limitations, only %15 random entries from each dataset is used for calculations.

Twitter text
```{r}
con <- file("./en_US.twitter.txt", "r") 
twit = readLines(con, skipNul = T)
close(con)

#takin 15% of data for memory reasons
set.seed(1)
x = sample(2360148, 300000, replace = F)

train = twit[x]

TW <- corpus(train)
rm(con, train, twit, x)
summary(TW, 5)

y = list (x1 = 1:350000,
          x2 = 350001:700000,
          x3 = 700001:1050000,
          x4 = 1050001:1400000,
          x5 = 1400000:1750000,
          x6 = 1750001:2100000,
          x7 = 2100001:2360148)
```

Blog text
```{r}
con <- file("./en_US.blogs.txt", "r") 
blog = readLines(con, skipNul = T)
close(con)

#takin 15% of data for memory reasons
set.seed(12345)
x = sample(899288, 135000, replace = F)

train = blog[x]

BG <- corpus(train)
rm(con, train, blog, x)
summary(BG, 5)
```

News text

```{r}
# 1010242
con <- file("./en_US.news.txt", "r") 
news = readLines(con, skipNul = T)
close(con)

#takin 15% of data for memory reasons
set.seed(10000)
x = sample(1010242, 150000, replace = F)

train = news[x]

NS <- corpus(train)
rm(con, train, news, x)
summary(NS, 5)
```



# Visualize Word Frequency


```{r}
BG.uni = getTables(dat = BG, ng = 1)
set.seed(100)
textplot_wordcloud(BG.uni, min_count = 3000, random_order = FALSE,
                   rotation = .25,
                   color = RColorBrewer::brewer.pal(8, "Dark2"))
```


1. create frequency tables and n-grams
```{r}
getTables = function(dat, ng) {
    dat.dfm = dfm(dat, ngrams = ng, remove_punct = T, remove_numbers = T)
    dat.tbl = data.table(ngram = featnames(dat.dfm), freq = colSums(dat.dfm),
                         key = "ngram")
    dat.tbl = dat.tbl[order(ngram)]
    return(dat.tbl)
}

getTablesRM = function(dat, ng) {
    dat.dfm = dfm(dat, ngrams = ng, remove_punct = T, remove_numbers = T,
                  remove = stopwords("english"))
    dat.tbl = data.table(ngram = featnames(dat.dfm), freq = colSums(dat.dfm),
                         key = "ngram")
    dat.tbl = dat.tbl[order(ngram)]
    return(dat.tbl)
}

```

unigram, bigram, and trigrams

```{r}
BG.unigs = getTables(BG, 1)
BG.bigrs = getTables(BG, 2)
BG.trigs = getTables(BG, 3)
rm(BG)

NS.unigs = getTables(NS, 1)
NS.bigrs = getTables(NS, 2)
NS.trigs = getTables(NS, 3)
rm(NS)

unigs = getTables(TW, 1)
unigsfull = getTablesRM(TW, 1)
bigrs = getTables(TW, 2)
trigs = getTables(TW, 3)
rm(TW)
gc()
```


# Algorithm


Probs of words completing OBSERVED trigrams
```{r}
bigPre <- 'is_book'

## Returns a two column data.table of observed trigrams that start with the
## bigram prefix (bigPre) in the first column named ngram and
## frequencies/counts in the second column named freq. If no observed trigrams
## that start with bigPre exist, an empty data.table is returned.
##
## bigPre -  single-element char array of the form w2_w1 which are the first 
##           two words of the trigram we are predicting the tail word of
## trigrams - 2 column data.table. The first column: ngram,
##            contains all the trigrams in the corpus. The second column:
##            freq, contains the frequency/count of each trigram.
getODTrigs = function(bigPre, trigrams) {
    regex = sprintf("%s%s%s", "^", bigPre, "_")
    tri.obs = trigrams[ngram %like% regex]
    return(tri.obs)
}



## Returns a two column data.table of observed bigrams that start with the
## unigram prefix (second word of bigPre) in the first column named ngram and
## frequencies/counts in the second column named freq. If no observed bigrams
## that start with second word of bigPre exist, an empty data.table is returned.
##
## bigPre -  single-element char array of the form w2_w1 which are the first 
##           two words of the trigram we are predicting the tail word of
## trigrams - 2 column data.table. The first column: ngram,
##            contains all the trigrams in the corpus. The second column:
##            freq, contains the frequency/count of each trigram.
getODBigs = function(bigPre, bigrams) {
    w_i_1 = str_split(bigPre, "_")[[1]][2]
    regex = sprintf("%s%s%s", "^", w_i_1, "_")
    big.obs = bigrams[ngram %like% regex]
    return(big.obs)
}



#x = getTrigProbs(bigPre, trigs, bigrs, unigs)

getTrigProbs = function(bigPre, trigrams, bigrams, unigrams) {
        obs_trigs = getODTrigs(bigPre, trigrams)
        regex = sprintf("%s%s%s", "^", bigPre, "$")
        #bigr_count = bigrams[grep(regex, bigrams$ngram), ]$freq
        bigr_count = bigrams[ngram %like% regex]$freq
        count = length(strsplit(bigPre, "_")[[1]])
        if(count < 2 | nrow(obs_trigs) < 1) {
                bigr_probs = getBigProbs(bigPre, bigrams, unigrams)
                probs = data.table(ngram = bigr_probs$ngram, 
                                   prob = bigr_probs$prob * 0.4)
                return(probs)
        } else {
                probs = obs_trigs %>% rename( prob = freq) %>%
                    mutate(prob = prob / bigr_count) %>% 
                    arrange(desc(prob), ngram)
                return(probs)
        }
        
}


getBigProbs = function(bigPre, bigrams, unigrams) {
    obs_bigs = getODBigs(bigPre, bigrams)
    w_i_1 <- str_split(bigPre, "_")[[1]][2]
    regex = sprintf("%s%s%s", "^", w_i_1, "$")
    #unig_count = unigrams[grep(regex, unigrams$ngram), ]$freq
    unig_count = unigrams[ngram %like% regex]$freq
    if(nrow(obs_bigs) < 1) {
        probs = getUnigProbs(unigrams) %>% mutate(prob = freq * 0.4)
        return(probs)
    } else {
        probs = obs_bigs %>% rename(prob = freq) %>%
            mutate(prob = prob/unig_count) %>%
            arrange(desc(prob), ngram)
        return(probs)
    }
}

#to get the second word of each row
getAdjNgram = function(my.ngram) {
    samp = strsplit(my.ngram[1,ngram], split = "_")
    if(length(samp[[1]]) == 3) {
        my.ngram$ngram = str_split_fixed(my.ngram$ngram, "_", 3)[, 3]
        return(my.ngram)
    } else if(length(samp[[1]]) == 2) {
        my.ngram$ngram = str_split_fixed(my.ngram$ngram, "_", 2)[, 2]
        return(my.ngram)
    } else return(my.ngram)
}

#first_words <- str_split_fixed(obsBoBigrams$ngram, "_", 2)[, 1]

getSQLFinalProbs = function(bigPre, unigs) {
    ID = integer()
    querry = paste("SELECT * FROM trigs WHERE ngram like '", 
                   bigPre, "_%' ORDER BY freq DESC", sep = "")
    my.dat = as.data.table(sqldf(querry))
    if(nrow(my.dat) < 1) {
        my.dat = getSQLBigProbs(bigPre, bigrs)
        if(nrow(my.dat) < 1) {
            my.dat = getUnigProbs(unigs)[, freq := freq * 0.16]
            return(my.dat)
        } else {
            my.dat = my.dat[, freq := freq * 0.4]
            return(my.dat)}
    } else {
        return(my.dat)
    }
}


getSQLTrigProbs = function(bigPre, trigrams) {
    querry = paste("SELECT * FROM trigs WHERE ngram like '", 
                   bigPre, "_%' ORDER BY freq DESC", sep = "")
    my.dat = as.data.table(sqldf(querry))
    w_i_1 <- str_split(bigPre, "_")[[1]][2]
    querry = paste("SELECT * FROM bigrs WHERE ngram like '",
                   w_i_1, "_%' ORDER BY freq DESC", sep = "")
    my.dat.bigs = as.data.table(sqldf(querry))
    total_count = my.dat.bigs[, sum(freq)]
    my.dat = my.dat[ , freq := freq / total_count]
    return(my.dat)
}

getSQLBigProbs = function(bigPre, bigrams) {
    w_i_1 <- str_split(bigPre, "_")[[1]][2]
    querry = paste("SELECT * FROM bigrs WHERE ngram like '",
                   w_i_1, "_%' ORDER BY freq DESC", sep = "")
    my.dat = as.data.table(sqldf(querry))
    querry = paste("SELECT * FROM unigsFull WHERE ngram like '",
                   w_i_1, "' ORDER BY freq DESC", sep = "")
    my.dat.unig = as.data.table(sqldf(querry))
    total_count = my.dat.unig[, sum(freq)]
    my.dat = my.dat[, freq := freq/total_count]
    return(my.dat)
}

getUnigProbs = function(unigrams) {
    #probs = unigrams %>% rename(prob = freq) %>%
    #    mutate(prob = prob/sum(prob)) %>%
    #    arrange(desc(prob), ngram)
    probs = data.table(unigrams)
    total_count = probs[, sum(freq)]
    probs = probs[order(freq, decreasing = T)][1:50][, freq := freq/total_count]
    return(probs)
}




x = sqldf("SELECT * FROM trigs WHERE ngram like 'settle_the_%' ORDER BY freq DESC")

my.dat = sqldf("SELECT * FROM trigs WHERE ngram like 'how_is_%' ORDER BY freq DESC")

mbm <- microbenchmark(
    "a" = {x <- getTrigProbs(bigPre, trigs, bigrs, unigs)},
    "b" = {y <- getSQLFinalProbs(bigPre, trigs)},
    times = 50
)


```


test

```{r}
#performance test with unobserved regex
bigPre = "kalem_kagit"
mbm <- microbenchmark(
    "a" = {x <- getTrigProbs(bigPre, trigs, bigrs, unigs)},
    "b" = {y <- getSQLFinalProbs(bigPre, trigs)},
    times = 50
)
mbm
```

```{r}
#performance test with observed regex
bigPre = "how_is"
mbm <- microbenchmark(
    "a" = {x <- getTrigProbs(bigPre, trigs, bigrs, unigs)},
    "b" = {y <- getSQLFinalProbs(bigPre, trigs)},
    times = 50
)
mbm
```

