---
title: "Main1"
author: "Yigit Ozan Berk"
date: "11/1/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
##orig.wd <- getwd()
##setwd("../final/en_US")
numwords <- system("wc -w *.txt", intern=TRUE)  # intern=TRUE to return output  
numlines <- system("wc -l *.txt", intern=TRUE)
numbytes <- system("wc -c *.txt", intern=TRUE)
# longest <- system("wc -L *.txt", intern=TRUE) # not working
##setwd(orig.wd)  # return to original working dir, ie. the parent of /final

# number of words for each dataset
blog.numwords <- as.numeric(gsub('[^0-9]', '', numwords[1]))
news.numwords <- as.numeric(gsub('[^0-9]', '', numwords[2]))
twit.numwords <- as.numeric(gsub('[^0-9]', '', numwords[3]))

# number of lines for each dataset
blog.numlines <- as.numeric(gsub('[^0-9]', '', numlines[1]))
news.numlines <- as.numeric(gsub('[^0-9]', '', numlines[2]))
twit.numlines <- as.numeric(gsub('[^0-9]', '', numlines[3]))

# number of bytes for each dataset
blog.numbytes <- as.numeric(gsub('[^0-9]', '', numbytes[1]))
news.numbytes <- as.numeric(gsub('[^0-9]', '', numbytes[2]))
twit.numbytes <- as.numeric(gsub('[^0-9]', '', numbytes[3]))
```


# Data import 

```{r}
install.packages("tm")
install.packages("slam")
library(slam)
library(tm)

```

## twitter files
```{r}
con <- file("./en_US.twitter.txt", "r") 
twit = readLines(con, skipNul = T)
close(con)
y = NULL
for(i in 1:length(twit)) {y[i] = nchar(twit[[i]])}
summary(y)
```


```{r}
set.seed(1)
x = rbinom(2360148, size = 1, prob = .7)
#70% of data for training, 30% for testing
train = twit[x]
test = twit[!x]
rm(twit)
```

## blogs

```{r}
con <- file("./en_US.blogs.txt", "r") 
blogs = readLines(con, skipNul = T)
close(con)

y = NULL
for(i in 1:length(blogs)) {y[i] = nchar(blogs[[i]])}
summary(y)

set.seed(123)
x = rbinom(899288, size = 1, prob = .7)
#70% of data for training, 30% for testing
train = blog[x]
test = blog[!x]
```


## news

```{r}
con <- file("./en_US.news.txt", "r") 
news = readLines(con, skipNul = T)
close(con)

set.seed(12345)
x = sample(1010242, 50000, replace = F)
#5% of data for training, 70% for testing
#due to hardware issues
train = news[x]
test = news[!x]
rm(con, news)
```

## Loading the data 

Loading Data
Text to be mined can be loaded into R from different source formats.It can come from text files(.txt),pdfs (.pdf),csv files(.csv) e.t.c ,but no matter the source format ,to be used in the tm package it is turned into a “corpus”.
A corpus is defined as “a collection of written texts, especially the entire works of a particular author or a body of writing on a particular subject”.
The tm package use the Corpus() function to create a corpus.

```{r}
#Load data as corpus
#VectorSource() creates character vectors
myN <- Corpus(VectorSource(train))
```

# Cleaning

1. converting everything to lower case
2. remove punctuation
3.remove extra whitespace
4. remove stopwords
5. remove numbers
remove whitespace again due to spaces of numbers

```{r}
#1
myN = tm_map(myN, content_transformer(tolower))
#2
myN = tm_map(myN, content_transformer(removePunctuation))
#3
myN = tm_map(myN, content_transformer(stripWhitespace))
#4
myN = tm_map(myN, removeWords, stopwords("english"))
#5
myN = tm_map(myN, removeNumbers)
#6
myN = tm_map(myN, content_transformer(stripWhitespace))
```



```{r}
# convert to lower case
mydata <- tm_map(mydata, content_transformer(tolower))
#remove ������ what would be emojis
mydata<-tm_map(mydata, content_transformer(gsub), pattern="\\W",replace=" ")
# remove URLs
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
mydata <- tm_map(mydata, content_transformer(removeURL)
)
# remove anything other than English letters or space
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
mydata <- tm_map(mydata, content_transformer(removeNumPunct))
# remove stopwords
mydata <- tm_map(mydata, removeWords, stopwords("english"))
#u can create custom stop words using the code below.
#myStopwords <- c(setdiff(stopwords('english'), c("r", "big")),"use", "see", "used", "via", "amp")
#mydata <- tm_map(mydata, removeWords, myStopwords)
# remove extra whitespace
mydata <- tm_map(mydata, stripWhitespace)
# Remove numbers
mydata <- tm_map(mydata, removeNumbers)
# Remove punctuations
mydata <- tm_map(mydata, removePunctuation)
```



## Stemming

```{r}
install.packages("SnowballC")
library(SnowballC)
myN = tm_map(myN, stemDocument)
```

# Term-Document Matrix

```{r}
#create a term matrix and store it as dtm
newsTDM <- TermDocumentMatrix(myN)
```

## high freq words

```{r}
newsTDMHighFreq = findFreqTerms(newsTDM, lowfreq = 2500, highfreq = Inf)
inspect(newsTDM[newsTDMHighFreq, 1:10 ])
newsTDMHighFreq
```

```{r}
news.freq <- row_sums(newsTDM, na.rm=TRUE)
library(ggplot2)
#top 10 words -- counts
qplot(y = news.freq[order(news.freq, decreasing = T)][1:50], x = names(x = news.freq[order(news.freq, decreasing = T)][1:50]), ylab= "Frequency", xlab = "Word", main = "The top 50 words -- counts")
```

```{r}
news.freq[order(news.freq, decreasing = T)][1:50]
summary(news.freq)
```

```{r}
quantile4 = findFreqTerms(newsTDM, lowfreq = 5, highfreq = Inf)
names50 = names(news.freq[order(news.freq, decreasing = T)][1:50])
tdm = newsTDM[names50,]
tdm2 = newsTDM[quantile4, ]
tdm2.freq = row_sums(tdm2, na.rm = T)
tdm2.freq.ord = tdm2.freq[order(tdm2.freq, decreasing = T)]
summary(tdm2.freq)

```

```{r}
plot(tdm2.freq.ord[1:1000])
```
```{r}
tdm2.freq.log = log(tdm2.freq.ord)
plot(tdm2.freq.log)
```

```{r}
findAssocs(tdm2, "team", 0.05)
```
```{r}
findAssocs(tdm2, "season", 0.05)
```

Maybe I can use the 50 most used words to create <50 classes by checking the word associations with findAssocs() function. Then, If the word is found in one of the class set of words, that class weights will be used in the prediction algorithm.

*quanteda package*